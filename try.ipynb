{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"The easiest way to earn points with Fetch Rewards is to just shop for the products you already love. If you have any participating brands on your receipt, you'll get points based on the cost of the products. You don't need to clip any coupons or scan individual barcodes. Just scan each grocery receipt after you shop and we'll find the savings for you.\"\n",
    "text2 = \"The easiest way to earn points with Fetch Rewards is to just shop for the items you already buy. If you have any eligible brands on your receipt, you will get points based on the total cost of the products. You do not need to cut out any coupons or scan individual UPCs. Just scan your receipt after you check out and we will find the savings for you.\"\n",
    "text3 = \"We are always looking for opportunities for you to earn more points, which is why we also give you a selection of Special Offers. These Special Offers are opportunities to earn bonus points on top of the regular points you earn every time you purchase a participating brand. No need to pre-select these offers, we'll give you the points whether or not you knew about the offer. We just think it is easier that way.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define basic stopwords list\n",
    "stopwords = [\n",
    "    'if',\n",
    "    'and',\n",
    "    'the',\n",
    "    'as',\n",
    "    'like',\n",
    "    'to',\n",
    "    'you',\n",
    "    'i',\n",
    "    'we',\n",
    "    'me',\n",
    "    'that',\n",
    "    'this',\n",
    "    'is',\n",
    "    'for',\n",
    "    'of',\n",
    "    'my',\n",
    "    'has',\n",
    "    'a',\n",
    "    'an',\n",
    "    'at',\n",
    "    'out',\n",
    "    'on',\n",
    "    'or',\n",
    "    'your',\n",
    "    'our',\n",
    "    'do',\n",
    "    'not',\n",
    "    'yes',\n",
    "    'no',\n",
    "    'these',\n",
    "    'those',\n",
    "    'dont',    \n",
    "]\n",
    "\n",
    "advanced_stopwords = [\n",
    "    'have',\n",
    "    'just',\n",
    "    'any',\n",
    "    'who',\n",
    "    'what',\n",
    "    'where',\n",
    "    'why',\n",
    "    'how',\n",
    "    'with',\n",
    "    'from',\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity of raw text words with punctuation removed\n",
      "[0.3684210526315789, 0.8076923076923077, 0.8148148148148148]\n"
     ]
    }
   ],
   "source": [
    "text1 = \"The easiest way to earn points with Fetch Rewards is to just shop for the products you already love. If you have any participating brands on your receipt, you'll get points based on the cost of the products. You don't need to clip any coupons or scan individual barcodes. Just scan each grocery receipt after you shop and we'll find the savings for you.\"\n",
    "text2 = \"The easiest way to earn points with Fetch Rewards is to just shop for the items you already buy. If you have any eligible brands on your receipt, you will get points based on the total cost of the products. You do not need to cut out any coupons or scan individual UPCs. Just scan your receipt after you check out and we will find the savings for you.\"\n",
    "text3 = \"We are always looking for opportunities for you to earn more points, which is why we also give you a selection of Special Offers. These Special Offers are opportunities to earn bonus points on top of the regular points you earn every time you purchase a participating brand. No need to pre-select these offers, we'll give you the points whether or not you knew about the offer. We just think it is easier that way.\"\n",
    "\n",
    "text1,text2,text3 = (\n",
    "    [\n",
    "        ''.join(['' if letter in string.punctuation else letter for letter in text]).lower().split(' ')\n",
    "        for text in [text1,text2,text3]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# just the text without punctuation\n",
    "print('similarity of raw text words with punctuation removed')\n",
    "print(\n",
    "    [\n",
    "        (len(set(a+b)) - len(set(a).intersection(set(b)))) / len(set(a+b))\n",
    "        for a,b in [\n",
    "            (text1,text2),\n",
    "            (text1,text3),\n",
    "            (text2,text3)\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'that makes little sense - perhaps stopwords are the main driver of this'"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'that makes little sense - perhaps stopwords are the main driver of this'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity without stopwords\n",
      "[0.6097560975609756, 0.11666666666666667, 0.08064516129032258]\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords from each text\n",
    "temp1,temp2,temp3 = (\n",
    "    [word for word in text if not word in stopwords]\n",
    "    for text in [text1,text2,text3]\n",
    ")\n",
    "\n",
    "# without stopwords\n",
    "print('similarity without stopwords')\n",
    "print(\n",
    "    [\n",
    "        (len(set(a+b)) - len(set(a)-set(b)) - len(set(b)-set(a))) / len(set(a+b))\n",
    "        for a,b in [\n",
    "            (temp1,temp2),\n",
    "            (temp1,temp3),\n",
    "            (temp2,temp3)\n",
    "        ]\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'that is much better'"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'that is much better'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity without \"advanced\" stopwords\n",
      "[0.6037735849056604, 0.1917808219178082, 0.18421052631578946]\n"
     ]
    }
   ],
   "source": [
    "# remove advanced stopwords\n",
    "\n",
    "temp1,temp2,temp3 = (\n",
    "    [word for word in text if not word in advanced_stopwords]\n",
    "    for text in [text1,text2,text3]\n",
    ")\n",
    "\n",
    "# without stopwords\n",
    "print('similarity without \"advanced\" stopwords')\n",
    "print(\n",
    "    [\n",
    "        (len(set(a+b)) - len(set(a)-set(b)) - len(set(b)-set(a))) / len(set(a+b))\n",
    "        for a,b in [\n",
    "            (temp1,temp2),\n",
    "            (temp1,temp3),\n",
    "            (temp2,temp3)\n",
    "        ]\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this makes the second two a lot more alike, which makes  sense as they're 'fundamentally' similar\""
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"this makes the second two a lot more alike, which makes  sense as they're 'fundamentally' similar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of with and without stopwords\n",
      "[0.6176762661370407, 0.19204425711275025, 0.1846978557504873]\n"
     ]
    }
   ],
   "source": [
    "# mean of with all stopwords and without all stopwords\n",
    "print('mean of with and without stopwords')\n",
    "print(\n",
    "    [\n",
    "        (x+y)/2 for x,y in zip(\n",
    "            [\n",
    "                (len(set(a+b)) - len(set(a)-set(b)) - len(set(b)-set(a))) / len(set(a+b))\n",
    "                for a,b in [\n",
    "                    (temp1,temp2),\n",
    "                    (temp1,temp3),\n",
    "                    (temp2,temp3)\n",
    "                ]\n",
    "            ],[\n",
    "                (len(set(a+b)) - len(set(a)-set(b)) - len(set(b)-set(a))) / len(set(a+b))    \n",
    "                for a,b in [\n",
    "                    (text1,text2),\n",
    "                    (text1,text3),\n",
    "                    (text2,text3)\n",
    "                ]\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a good balance, if even philosophically rather than practically'"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'this is a good balance, if even philosophically rather than practically'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all words similarity - with stopwords\n",
      "0.39849624060150374\n",
      "0.17985611510791366\n",
      "0.18055555555555555\n",
      "\n",
      "all words similarity - without stopwords\n",
      "0.05263157894736842\n",
      "0.375\n",
      "0.18055555555555555\n",
      "\n",
      "all words similarity - without advanced stopwords\n",
      "0.05263157894736842\n",
      "0.375\n",
      "0.18055555555555555\n"
     ]
    }
   ],
   "source": [
    "# number of ALL words that are shared, including duplicates, divided by total number of words in both texts\n",
    "\n",
    "text1 = \"The easiest way to earn points with Fetch Rewards is to just shop for the products you already love. If you have any participating brands on your receipt, you'll get points based on the cost of the products. You don't need to clip any coupons or scan individual barcodes. Just scan each grocery receipt after you shop and we'll find the savings for you.\"\n",
    "text2 = \"The easiest way to earn points with Fetch Rewards is to just shop for the items you already buy. If you have any eligible brands on your receipt, you will get points based on the total cost of the products. You do not need to cut out any coupons or scan individual UPCs. Just scan your receipt after you check out and we will find the savings for you.\"\n",
    "text3 = \"We are always looking for opportunities for you to earn more points, which is why we also give you a selection of Special Offers. These Special Offers are opportunities to earn bonus points on top of the regular points you earn every time you purchase a participating brand. No need to pre-select these offers, we'll give you the points whether or not you knew about the offer. We just think it is easier that way.\"\n",
    "\n",
    "text1,text2,text3 = (\n",
    "    [\n",
    "        ''.join(['' if letter in string.punctuation else letter for letter in text]).lower().split(' ')\n",
    "        for text in [text1,text2,text3]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# unique words count similarity with stopwords\n",
    "print('all words similarity - with stopwords')\n",
    "for a,b in [(text1,text2),(text1,text3),(text2,text3)]:\n",
    "    counts_a, counts_b = {},{}\n",
    "    shared = []\n",
    "    \n",
    "    # for each text, get the counts of every unique word\n",
    "    for word in a:\n",
    "        counts_a[word] = counts_a.get(word,0)+1\n",
    "    for word in b:\n",
    "        counts_b[word] = counts_b.get(word,0)+1\n",
    "        \n",
    "    # get all shared words for the two texts\n",
    "    for word in set(a).intersection(set(b)):\n",
    "        shared.extend([ word for x in range(min(counts_a[word],counts_b[word]))])\n",
    "            \n",
    "    print(len(shared)/(len(a)+len(b)))\n",
    "\n",
    "    \n",
    "\n",
    "# remove stopwords\n",
    "print('\\nall words similarity - without stopwords')\n",
    "text1,text1,text1 = (\n",
    "    [word for word in text if not word in stopwords]\n",
    "    for text in [text1,text2,text3]\n",
    ")\n",
    "\n",
    "for a,b in [(text1,text2),(text1,text3),(text2,text3)]:\n",
    "    counts_a, counts_b = {},{}\n",
    "    shared = []\n",
    "    \n",
    "    # for each text, get the counts of every unique word\n",
    "    for word in a:\n",
    "        counts_a[word] = counts_a.get(word,0)+1\n",
    "    for word in b:\n",
    "        counts_b[word] = counts_b.get(word,0)+1\n",
    "        \n",
    "    # get all shared words for the two texts\n",
    "    for word in set(a).intersection(set(b)):\n",
    "        shared.extend([ word for x in range(min(counts_a[word],counts_b[word]))])\n",
    "            \n",
    "    print(len(shared)/(len(a)+len(b)))\n",
    "\n",
    "    \n",
    "# remove advanced stopwords\n",
    "print('\\nall words similarity - without advanced stopwords')\n",
    "text1,text1,text1 = (\n",
    "    [word for word in text if not word in stopwords]\n",
    "    for text in [text1,text2,text3]\n",
    ")\n",
    "\n",
    "for a,b in [(text1,text2),(text1,text3),(text2,text3)]:\n",
    "    counts_a, counts_b = {},{}\n",
    "    shared = []\n",
    "    \n",
    "    # for each text, get the counts of every unique word\n",
    "    for word in a:\n",
    "        counts_a[word] = counts_a.get(word,0)+1\n",
    "    for word in b:\n",
    "        counts_b[word] = counts_b.get(word,0)+1\n",
    "        \n",
    "    # get all shared words for the two texts\n",
    "    for word in set(a).intersection(set(b)):\n",
    "        shared.extend([ word for x in range(min(counts_a[word],counts_b[word]))])\n",
    "            \n",
    "    print(len(shared)/(len(a)+len(b)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this doesn't really make sense as it includes too many of the stopwords and near-stopwords\""
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"this doesn't really make sense as it includes too many of the stopwords and near-stopwords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram similarity [44, 5, 5]\n",
      "bigram options [4284, 4662, 5032]\n",
      "trigram similarity [31, 0, 0]\n",
      "trigram options [4154, 4526, 4891]\n"
     ]
    }
   ],
   "source": [
    "# similarity of random bigrams (testing order and phrases)\n",
    "\n",
    "# get bigrams\n",
    "bigram1,bigram2,bigram3 = ([[text[i],text[i+1]] for i in range(len(text)-1)] for text in [text1,text2,text3])\n",
    "\n",
    "print('bigram similarity',[sum([1 for bg2 in a for bg1 in b if bg1==bg2]) for a,b in [(bigram1,bigram2),(bigram1,bigram3),(bigram2,bigram3)]])\n",
    "print('bigram options',[sum([1 for bg2 in a for bg1 in b]) for a,b in [(bigram1,bigram2),(bigram1,bigram3),(bigram2,bigram3)]])\n",
    "\n",
    "# similarity of random trigrams\n",
    "\n",
    "# get trigrams\n",
    "trigram1,trigram2,trigram3 = ([[text[i],text[i+1],text[i+2]] for i in range(len(text)-2)] for text in [text1,text2,text3])\n",
    "\n",
    "print('trigram similarity',[sum([1 for tg2 in a for tg1 in b if tg1==tg2]) for a,b in [(trigram1,trigram2),(trigram1,trigram3),(trigram2,trigram3)]])\n",
    "print('trigram options',[sum([1 for tg2 in a for tg1 in b]) for a,b in [(trigram1,trigram2),(trigram1,trigram3),(trigram2,trigram3)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"these seem like they should be useful but it's hard to scale them 0 to 1...let's ignore it\""
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"these seem like they should be useful but it's hard to scale them 0 to 1...let's ignore it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading from file\n",
    "path1,path2,path3 = ('text{}.txt'.format(n) for n in range(1,4))\n",
    "\n",
    "\n",
    "with open(path1,'r') as file:\n",
    "    text1 = file.read().replace('\\n', '')\n",
    "\n",
    "with open(path2,'r') as file:\n",
    "    text2 = file.read().replace('\\n', '')\n",
    "\n",
    "with open(path3,'r') as file:\n",
    "    text3 = file.read().replace('\\n', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The easiest way to earn points with Fetch Rewards is to just shop for the items you already buy. If you have any eligible brands on your receipt, you will get points based on the total cost of the products. You do not need to cut out any coupons or scan individual UPCs. Just scan your receipt after you check out and we will find the savings for you.'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The easiest way to earn points with Fetch Rewards is to just shop for the items you already buy. If you have any eligible brands on your receipt, you will get points based on the total cost of the products. You do not need to cut out any coupons or scan individual UPCs. Just scan your receipt after you check out and we will find the savings for you.'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We are always looking for opportunities for you to earn more points, which is why we also give you a selection of Special Offers. These Special Offers are opportunities to earn bonus points on top of the regular points you earn every time you purchase a participating brand. No need to pre-select these offers, we'll give you the points whether or not you knew about the offer. We just think it is easier that way.\""
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full basic implementation\n",
    "\n",
    "def fetchilarity_score(path1,path2):\n",
    "\n",
    "    # define basic stopwords list\n",
    "    stopwords = [\n",
    "        'if','and','the','as','like','to',\n",
    "        'you','i','we','me','that','this',\n",
    "        'is','for','of','my','has','a',\n",
    "        'an','at','out','on','or','your',\n",
    "        'our','do','not','yes','no','these',\n",
    "        'those','dont',\n",
    "        # including \"advanced\" stopwords\n",
    "        'have','just','any','who','what',\n",
    "        'where','why','how','with','from'\n",
    "    ]\n",
    "    \n",
    "    # get the files\n",
    "    with open(path1,'r') as file:\n",
    "        text1 = file.read().replace('\\n', '')\n",
    "\n",
    "    with open(path2,'r') as file:\n",
    "        text2 = file.read().replace('\\n', '')\n",
    "        \n",
    "\n",
    "    # remove punctuation\n",
    "    text1,text2 = (\n",
    "        [\n",
    "            ''.join(['' if letter in string.punctuation else letter for letter in text]).lower().split(' ')\n",
    "            for text in [text1,text2]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    \n",
    "    # get similarity score of raw text words    \n",
    "    fetchilarity_raw = len(set(text1).intersection(set(text2))) / len(set(text1+text2))\n",
    "    \n",
    "\n",
    "    # get similarity score with all stopwords removed\n",
    "    text1,text2 = (\n",
    "        [word for word in text if not word in stopwords]\n",
    "        for text in [text1,text2]\n",
    "    )\n",
    "\n",
    "    \n",
    "    # without stopwords\n",
    "    fetchilarity_stopwords_removed = len(set(text1).intersection(set(text2))) / len(set(text1+text2))\n",
    "    \n",
    "    \n",
    "    # overall score\n",
    "    fetchilarity = (fetchilarity_raw + fetchilarity_stopwords_removed)/2\n",
    "    \n",
    "    \n",
    "    return fetchilarity\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.947741935483871"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path1 = 'text2.txt'\n",
    "path2 = 'text1.txt'\n",
    "\n",
    "\n",
    "fetchilarity_score(path1,path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
